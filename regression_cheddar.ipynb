{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ea25cdf7-bdbc-3cf1-0737-bc51675e3374",
    "_uuid": "fed5696c67bf55a553d6d04313a77e8c617cad99"
   },
   "source": [
    "# Regresión: Advance Techniques in Python\n",
    "\n",
    "\n",
    "### Regresión y visualización del dataset Cheddar de Faraway. \n",
    "\n",
    "En este notebook explicamos un workflow típico para analizar algún dataset y utilizamos diversas ténicas de regresión.\n",
    "\n",
    "Debido a que las diversas técnicas utilizadas se encuentran fuera del alcance de nuestro curso de Modelos Estádisticos, no serán explicadas. Sin embargo, incluimos links y recursos para poder entender los diversos modelos, técnicas y pasos.\n",
    "\n",
    "## Workflow stages\n",
    "\n",
    "El workflow que debemos seguir dado un dataset debe seguir los siguientes pasos descritos en el libro [Data Science Solutions](https://www.amazon.com/Data-Science-Solutions-Startup-Workflow/dp/1520545312):\n",
    "\n",
    "1. Question or problem definition.\n",
    "2. Acquire training and testing data.\n",
    "3. Wrangle, prepare, cleanse the data, outliers.\n",
    "4. Analyze, identify patterns, and explore the data.\n",
    "5. Model, predict and solve the problem.\n",
    "6. Visualize, report, and present the problem solving steps and final solution.\n",
    "7. Supply or submit the results.\n",
    "\n",
    "Aunque según el libro, hay varias etapas que pueden quebrantar los anteriores principios como son los siguientes:\n",
    "\n",
    "- We may combine mulitple workflow stages. We may analyze by visualizing data.\n",
    "- Perform a stage earlier than indicated. We may analyze data before and after wrangling.\n",
    "- Perform a stage multiple times in our workflow. Visualize stage may be used multiple times.\n",
    "- Drop a stage altogether. We may not need supply stage to productize or service enable our dataset for a competition.\n",
    "\n",
    "\n",
    "## Question and problem definition\n",
    "\n",
    "A veces los problemas son propuestos y están definidos previamente. Sin embargo, hay ocasiones en las que no hay un verdadero propósito y debemos obtener conclusiones de los datos presentes. Por ejemplo nosotros podemos preguntarnos en base a nuestro dataset las siguientes preguntas\n",
    "\n",
    "> ¿Existe correlacion entre algunas de las variables?, ¿Existen outliers en los datos aportados?, etc.\n",
    "\n",
    "Nuestro dataset contiene los siguientes campos:\n",
    "\n",
    "- **Taste:** una valoración subjetiva de los jueces.   \n",
    "- **Acetic:** la concentración de ácido acético en un queso de terminado en esca\n",
    "la logarítmica  \n",
    "- **H2S:** la concentración de sulfito de hidrógeno en escala logarítmica.\n",
    "- **Lactic:** Concentración de ácido láctico\n",
    "\n",
    "\n",
    "## Workflow goals\n",
    "\n",
    "La mayoría de las soluciones propuestas en el entorno de científico de datos pertence en una o varias de las siguientes cateogorías descrita en el libro antes mencionado:\n",
    "\n",
    "**Classifying.** We may want to classify or categorize our samples. We may also want to understand the implications or correlation of different classes with our solution goal.\n",
    "\n",
    "**Correlating.** One can approach the problem based on available features within the training dataset. Which features within the dataset contribute significantly to our solution goal? Statistically speaking is there a [correlation](https://en.wikiversity.org/wiki/Correlation) among a feature and solution goal? As the feature values change does the solution state change as well, and visa-versa? This can be tested both for numerical and categorical features in the given dataset. We may also want to determine correlation among features other than survival for subsequent goals and workflow stages. Correlating certain features may help in creating, completing, or correcting features.\n",
    "\n",
    "**Converting.** For modeling stage, one needs to prepare the data. Depending on the choice of model algorithm one may require all features to be converted to numerical equivalent values. So for instance converting text categorical values to numeric values.\n",
    "\n",
    "**Completing.** Data preparation may also require us to estimate any missing values within a feature. Model algorithms may work best when there are no missing values.\n",
    "\n",
    "**Correcting.** We may also analyze the given training dataset for errors or possibly innacurate values within features and try to corrent these values or exclude the samples containing the errors. One way to do this is to detect any outliers among our samples or features. We may also completely discard a feature if it is not contribting to the analysis or may significantly skew the results.\n",
    "\n",
    "**Creating.** Can we create new features based on an existing feature or a set of features, such that the new feature follows the correlation, conversion, completeness goals.\n",
    "\n",
    "**Charting.** How to select the right visualization plots and charts depending on nature of the data and the solution goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "5767a33c-8f18-4034-e52d-bf7a8f7d8ab8",
    "_uuid": "847a9b3972a6be2d2f3346ff01fea976d92ecdb6"
   },
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b5dc743-15b1-aac6-405e-081def6ecca1",
    "_uuid": "2d307b99ee3d19da3c1cddf509ed179c21dec94a"
   },
   "source": [
    "## Acquire data\n",
    "\n",
    "La librería de Pandas de Python permite manipular los datos de manera similar a los DataFrame de R. Empezamos leyendo los datos proporcionados por el paquete *Faraway* y los dividimos en dos partes (train y test) con una proporción de 80% de train y utilizamos una semilla para obtener siempre los mismos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e7319668-86fe-8adc-438d-0eef3fd0a982",
    "_uuid": "13f38775c12ad6f914254a08f0d1ef948a2bd453"
   },
   "outputs": [],
   "source": [
    "combine = pd.read_csv('cheddar.csv')\n",
    "\n",
    "# Divide into train and test\n",
    "train_df ,test_df = train_test_split(combine, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3d6188f3-dc82-8ae6-dabd-83e28fcbf10d",
    "_uuid": "79282222056237a52bbbb1dbd831f057f1c23d69"
   },
   "source": [
    "## Analyze by describing data\n",
    "\n",
    "Pandas nos ayuda a describir los datasets y resolver preguntas frecuentes acerca de las variables y datos en fases iniciales del proyecto.\n",
    "\n",
    "**¿Qué features continene nuestro dataset?**\n",
    "\n",
    "Observamos directamente los nombres de nuestras features para poder manipular y analizarlas. El dataframe cheddar consiste de 30 muestras de queso cheddar en las que se ha medido el sabor (Taste) y las concentraciones de ácido acético (Acetic), ácido sulfhídrico (H2S) y lactosa (Lactose).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ce473d29-8d19-76b8-24a4-48c217286e42",
    "_uuid": "ef106f38a00e162a80c523778af6dcc778ccc1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['taste' 'Acetic' 'H2S' 'Lactic']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd19a6f6-347f-be19-607b-dca950590b37",
    "_uuid": "1d7acf42af29a63bc038f14eded24e8b8146f541"
   },
   "source": [
    "**¿Qué categorias son cualitativas?**\n",
    "\n",
    "Estos valores clasifican nuestras observaciones/muestras de nuestro dataset de similar features. Entre las categorias cualitativas se encuntran: valores nominales, ordinales, ratios, intervalos, etc. Esta división nos ayuda a la visualización de gráficas. Algunos ejemplos son:\n",
    "\n",
    "- Categorical: Married, Sex, and Embarked. Ordinal: Position.\n",
    "\n",
    "**¿Cuáles son cuantitativas?**\n",
    "\n",
    "Las features que son numericas son valores númericos. Entre las cuales se encuentran valores discretos, continuos, temporales, etc. Algunos ejemplos son:\n",
    "\n",
    "- Continous: Age, Fare. Discrete: Year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "8d7ac195-ac1a-30a4-3f3f-80b8cf2c1c0f",
    "_uuid": "e068cd3a0465b65a0930a100cb348b9146d5fd2f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taste</th>\n",
       "      <th>Acetic</th>\n",
       "      <th>H2S</th>\n",
       "      <th>Lactic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.7</td>\n",
       "      <td>4.477</td>\n",
       "      <td>2.996</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.2</td>\n",
       "      <td>5.298</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>57.2</td>\n",
       "      <td>6.446</td>\n",
       "      <td>7.908</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>56.7</td>\n",
       "      <td>5.855</td>\n",
       "      <td>10.199</td>\n",
       "      <td>2.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>5.247</td>\n",
       "      <td>6.174</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    taste  Acetic     H2S  Lactic\n",
       "12    0.7   4.477   2.996    1.06\n",
       "21   15.2   5.298   5.220    1.33\n",
       "11   57.2   6.446   7.908    1.90\n",
       "23   56.7   5.855  10.199    2.01\n",
       "18   18.0   5.247   6.174    1.63"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8bfe9610-689a-29b2-26ee-f67cd4719079",
    "_uuid": "699c52b7a8d076ccd5ea5bc5d606313c558a6e8e"
   },
   "source": [
    "**¿Qué features contienen valores null?**\n",
    "\n",
    "Estos valores serán corregidos o eliminados.\n",
    "\n",
    "- Si son pocos datos, podrán ser eliminados.\n",
    "- Si son muchos podríamos intentar predecirlos utilzando algún modelo.\n",
    "\n",
    "**¿Cuáles son los tipos de datos para cada feature?**\n",
    "\n",
    "Entre los cuales se pueden encontrar double, float, integer, boolean, strings, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "9b805f69-665a-2b2e-f31d-50d87d52865d",
    "_uuid": "817e1cf0ca1cb96c7a28bb81192d92261a8bf427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24 entries, 12 to 13\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   taste   24 non-null     float64\n",
      " 1   Acetic  24 non-null     float64\n",
      " 2   H2S     24 non-null     float64\n",
      " 3   Lactic  24 non-null     float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 960.0 bytes\n",
      "________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 6 entries, 7 to 27\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   taste   6 non-null      float64\n",
      " 1   Acetic  6 non-null      float64\n",
      " 2   H2S     6 non-null      float64\n",
      " 3   Lactic  6 non-null      float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 240.0 bytes\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "print('_'*40)\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "859102e1-10df-d451-2649-2d4571e5f082",
    "_uuid": "2b7c205bf25979e3242762bfebb0e3eb2fd63010"
   },
   "source": [
    "**¿Cúal es la distribución numérica de los valores a lo largo de las observaciones?**\n",
    "\n",
    "Esto nos ayudará a entender el dataset y tener una visión global de nuestros datos y del dominio de nuestro problema.\n",
    "\n",
    "- Hay un total de 32 observaciones.\n",
    "- Todas las categorías son númericas continuas.\n",
    "- La puntuación media es de 26.55 y la más alta es 57.2\n",
    "- La mayoría de las valoraciones (> 75%) no pasan el aprobado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "58e387fe-86e4-e068-8307-70e37fe3f37b",
    "_uuid": "380251a1c1e0b89147d321968dc739b6cc0eecf2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taste</th>\n",
       "      <th>Acetic</th>\n",
       "      <th>H2S</th>\n",
       "      <th>Lactic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>26.558333</td>\n",
       "      <td>5.429417</td>\n",
       "      <td>5.965625</td>\n",
       "      <td>1.453333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.914257</td>\n",
       "      <td>0.559195</td>\n",
       "      <td>2.220955</td>\n",
       "      <td>0.313170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>4.477000</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.850000</td>\n",
       "      <td>5.216750</td>\n",
       "      <td>4.108500</td>\n",
       "      <td>1.257500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20.950000</td>\n",
       "      <td>5.389000</td>\n",
       "      <td>5.329000</td>\n",
       "      <td>1.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.925000</td>\n",
       "      <td>5.815250</td>\n",
       "      <td>7.599000</td>\n",
       "      <td>1.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>57.200000</td>\n",
       "      <td>6.446000</td>\n",
       "      <td>10.199000</td>\n",
       "      <td>2.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           taste     Acetic        H2S     Lactic\n",
       "count  24.000000  24.000000  24.000000  24.000000\n",
       "mean   26.558333   5.429417   5.965625   1.453333\n",
       "std    16.914257   0.559195   2.220955   0.313170\n",
       "min     0.700000   4.477000   2.996000   0.860000\n",
       "25%    13.850000   5.216750   4.108500   1.257500\n",
       "50%    20.950000   5.389000   5.329000   1.475000\n",
       "75%    38.925000   5.815250   7.599000   1.642500\n",
       "max    57.200000   6.446000  10.199000   2.010000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe() # Dataset description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69783c08-c8cc-a6ca-2a9a-5e75581c6d31",
    "_uuid": "a55f20dd6654610ff2d66c1bf3e4c6c73dcef9e5"
   },
   "source": [
    "## Model, predict and solve\n",
    "\n",
    "Now we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n",
    "\n",
    "- Logistic Regression\n",
    "- KNN or k-Nearest Neighbors\n",
    "- Support Vector Machines\n",
    "- Decision Tree\n",
    "- Random Forrest\n",
    "- Perceptron\n",
    "- Artificial neural network\n",
    "- RVM or Relevance Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "0acf54f9-6cf5-24b5-72d9-29b30052823a",
    "_uuid": "04d2235855f40cffd81f76b977a500fceaae87ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 3), (24,), (6, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df.drop(\"taste\", axis=1)\n",
    "Y_train = train_df[\"taste\"]\n",
    "X_test  = test_df.drop(\"taste\", axis=1).copy()\n",
    "X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "579bc004-926a-bcfe-e9bb-c8df83356876",
    "_uuid": "782903c09ec9ee4b6f3e03f7c8b5a62c00461deb"
   },
   "source": [
    "Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\n",
    "\n",
    "Note the confidence score generated by the model based on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0edd9322-db0b-9c37-172d-a3a4f8dec229",
    "_uuid": "a649b9c53f4c7b40694f60f5c8dc14ec5ef519ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Model\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LinearRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3af439ae-1f04-9236-cdc2-ec8170a0d4ee",
    "_uuid": "180e27c96c821656a84889f73986c6ddfff51ed3"
   },
   "source": [
    "We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n",
    "\n",
    "Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n",
    "\n",
    "- Taste and H2S has the greatest correlation.\n",
    "- Inversely, taste and Acetic has the lowest correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corrmat = combine.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac041064-1693-8584-156b-66674117e4d0",
    "_uuid": "ccba9ac0a9c3c648ef9bc778977ab99066ab3945"
   },
   "source": [
    "Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Although it is also used for regression using a soft output instead. Reference [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine).\n",
    "\n",
    "Note that the model generates a confidence score which is higher than Logistics Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "7a63bf04-a410-9c81-5310-bdef7963298f",
    "_uuid": "60039d5377da49f1aa9ac4a924331328bd69add1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.95"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machines\n",
    "from sklearn.svm import SVR\n",
    "svc = SVR()\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred = svc.predict(X_test)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "172a6286-d495-5ac4-1a9c-5b77b74ca6d2",
    "_uuid": "bb3ed027c45664148b61e3aa5e2ca8111aac8793"
   },
   "source": [
    "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).\n",
    "\n",
    "KNN confidence score is better than Logistics Regression but worse than SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "ca14ae53-f05e-eb73-201c-064d7c3ed610",
    "_uuid": "54d86cd45703d459d452f89572771deaa8877999"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.31"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e286e19-b714-385a-fcfa-8cf5ec19956a",
    "_uuid": "df148bf93e11c9ec2c97162d5c0c0605b75d9334"
   },
   "source": [
    "The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https://en.wikipedia.org/wiki/Perceptron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "ccc22a86-b7cb-c2dd-74bd-53b218d6ed0d",
    "_uuid": "c19d08949f9c3a26931e28adedc848b4deaa8ab6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46.84"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perceptron\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "perceptron = MLPRegressor()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "a4d56857-9432-55bb-14c0-52ebeb64d198",
    "_uuid": "52ea4f44dd626448dd2199cb284b592670b1394b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC\n",
    "from sklearn.svm import LinearSVR\n",
    "linear_svc = LinearSVR()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "Y_pred = linear_svc.predict(X_test)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "dc98ed72-3aeb-861f-804d-b6e3d178bf4b",
    "_uuid": "3a016c1f24da59c85648204302d61ea15920e740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd = SGDRegressor()\n",
    "sgd.fit(X_train, Y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bae7f8d7-9da0-f4fd-bdb1-d97e719a18d7",
    "_uuid": "1c70e99920ae34adce03aaef38d61e2b83ff6a9c"
   },
   "source": [
    "This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning).\n",
    "\n",
    "The model confidence score is the highest among models evaluated so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "dd85f2b7-ace2-0306-b4ec-79c68cd3fea0",
    "_uuid": "1f94308b23b934123c03067e84027b507b989e52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "85693668-0cd5-4319-7768-eddb62d2b7d0",
    "_uuid": "24f4e46f202a858076be91752170cad52aa9aefa"
   },
   "source": [
    "The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https://en.wikipedia.org/wiki/Random_forest).\n",
    "\n",
    "The model confidence score is the highest among models evaluated so far. We decide to use this model's output (Y_pred) for creating our competition submission of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "f0694a8e-b618-8ed9-6f0d-8c6fba2c4567",
    "_uuid": "483c647d2759a2703d20785a44f51b6dee47d0db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.95"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6c9eef8-83dd-581c-2d8e-ce932fe3a44d",
    "_uuid": "2c1428d022430ea594af983a433757e11b47c50c"
   },
   "source": [
    "### Model evaluation\n",
    "\n",
    "We can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "1f3cebe0-31af-70b2-1ce4-0fd406bcdfc6",
    "_uuid": "06a52babe50e0dd837b553c78fc73872168e1c7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>92.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>73.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>70.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>56.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>56.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>46.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>30.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model   Score\n",
       "7               Decision Tree  100.00\n",
       "3               Random Forest   92.95\n",
       "2         Logistic Regression   73.16\n",
       "1                         KNN   70.31\n",
       "5  Stochastic Gradient Decent   56.60\n",
       "6                  Linear SVC   56.28\n",
       "4                  Perceptron   46.84\n",
       "0     Support Vector Machines   30.95"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "aeec9210-f9d8-cd7c-c4cf-a87376d5f693",
    "_uuid": "cdae56d6adbfb15ff9c491c645ae46e2c91d75ce"
   },
   "source": [
    "## References\n",
    "\n",
    "This notebook has been created based on great work done solving the Titanic competition and other sources.\n",
    "\n",
    "- [A journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic)\n",
    "- [Getting Started with Pandas: Kaggle's Titanic Competition](https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests)\n",
    "- [Titanic Best Working Classifier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
